mutate(Comment = gsub("\\brt\\b|\\bRT\\b", "", Comment)) %>%
mutate(Comment = gsub("http://*", "", Comment)) %>%
unnest_tokens(paired_words, Comment, token = "ngrams", n = 2)
Donaldcomment_data_paired <- Donaldcommentdata %>%
dplyr::select(Comment) %>%
mutate(Comment = removeWords(Comment, stop_words$word)) %>%
mutate(Comment = gsub("\\brt\\b|\\bRT\\b", "", Comment)) %>%
mutate(Comment = gsub("http://*", "", Comment)) %>%
unnest_tokens(paired_words, Comment, token = "ngrams", n = 2)
# Load
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("jsonlite")
library("NCmisc")
# plotting and pipes - tidyverse!
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidytext)
# coupled words analysis
library(widyr)
# plotting packages
library(igraph)
library(ggraph)
options(stringsAsFactors = FALSE)
donaldData = stream_in(file("data/PUBATTLEGROUNDS.json"), pagesize = 5000)
donaldDataframe = as.data.frame(data)
plot(donaldDataframe$retrieved_on, donaldData$score)
Donaldcommentdata <- data.frame(User = donaldData$author,
Date = as.POSIXct(donaldData$created_utc, origin='1970-01-01'),
Comment = donaldData$body)
head(Donaldcommentdata)
Donaldminimum = min(Donaldcommentdata$Date)
Donaldmaximum = max(Donaldcommentdata$Date)
Donaldcommentdata = subset(Donaldcommentdata, User != "SavageAxeBot" & User != "KeepingdataDank" & User != "AutoModerator"
& User != "dataMods" & User != "BattleBusBot" & User != "MemeInvestor_bot" & User != "Transcribot"
& User != "Transcribot" & User != "commonmisspellingbot" & User != "TiltedTowersBot" & User != "stormshieldonebot"
& User != "WikiTextBot" & User != "RemindMeBot" & User != "thank_mr_skeltal_bot" & User != "societybot"
& User != "rick_rolled_bot" & User != "NoSkinBot")
Donaldcomments_top <- Donaldcommentdata %>%
dplyr::select(Comment) %>%
unnest_tokens(word, Comment)
#clean the top comments
Donaldcomments_top_clean <- Donaldcomments_top %>%
anti_join(as.data.frame(stop_words)) %>%
filter(!word == "shit") %>%
filter(!word == "gt") %>%
filter(!word == "https") %>%
filter(!word == "it's") %>%
filter(!word == "amp") %>%
filter(!word == "fucking") %>%
filter(!word == "fuck") %>%
filter(!word == "1") %>%
filter(!word == "2") %>%
filter(!word == "3") %>%
filter(!word == "removed") %>%
filter(!word == "deleted") %>%
filter(!word == "x200b")
Donaldtop_words <- Donaldcomments_top_clean %>%
count(word, sort = TRUE) %>%
top_n(100) %>%
mutate(word = reorder(word, n))
head(Donaldtop_words)
#word cloud
wordcloud(Donaldtop_words$word, Donaldtop_words$n, min.freq = 1,
max.words = 75, random.order = FALSE, rot.per = 0.15,
colors = brewer.pal(8, "Dark2"))
#frequency graph
Donaldcomments_top_clean %>%
count(word, sort = TRUE) %>%
top_n(30) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "Words",
y = "Count",
title = "Most commonly used english words in /r/the_donald, December 2018")
Donaldcomment_data_paired <- Donaldcommentdata %>%
dplyr::select(Comment) %>%
mutate(Comment = removeWords(Comment, stop_words$word)) %>%
mutate(Comment = gsub("\\brt\\b|\\bRT\\b", "", Comment)) %>%
mutate(Comment = gsub("http://*", "", Comment)) %>%
unnest_tokens(paired_words, Comment, token = "ngrams", n = 2)
Donald_Comments_Separated <- Donaldcomment_data_paired %>%
separate(paired_words, c("word1", "word2"), sep = " ") %>%
count(word1, word2, sort = TRUE)
head(Donald_Comments_Separated, 10)
Donald_Comments_Separated %>%
filter(n >= 80) %>%
filter(!word1 == "shit") %>%
filter(!word1 == "gt") %>%
filter(!word1 == "https") %>%
filter(!word1 == "it's") %>%
filter(!word1 == "amp") %>%
filter(!word1 == "fucking") %>%
filter(!word1 == "fuck") %>%
filter(!word1 == "1") %>%
filter(!word1 == "2") %>%
filter(!word1 == "3") %>%
filter(!word1 == "removed") %>%
filter(!word1 == "deleted") %>%
filter(!word1 == "x200b") %>%
filter(!word2 == "shit") %>%
filter(!word2 == "gt") %>%
filter(!word2 == "https") %>%
filter(!word2 == "it's") %>%
filter(!word2 == "amp") %>%
filter(!word2 == "fucking") %>%
filter(!word2 == "fuck") %>%
filter(!word2 == "1") %>%
filter(!word2 == "2") %>%
filter(!word2 == "3") %>%
filter(!word2 == "removed") %>%
filter(!word2 == "deleted") %>%
filter(!word2 == "x200b") %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(colour = "darkslategray4", size = 4) +
geom_node_text(aes(label = name), vjust = 1.2, size = 2.4) +
labs(title = "Redditor Comment links on /r/PUBATTLEGROUNDS",
subtitle = "January 2019",
x = "", y = "") +
theme_void()
# Load
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("jsonlite")
library("NCmisc")
# plotting and pipes - tidyverse!
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidytext)
# coupled words analysis
library(widyr)
# plotting packages
library(igraph)
library(ggraph)
options(stringsAsFactors = FALSE)
createGraphs <- function(data, subreddit, minimumCountLinks) {
dataframe = as.data.frame(data.frame(User = data$author,
Date = as.POSIXct(data$created_utc, origin='1970-01-01'),
Comment = data$body,
Score = data$score))
print(count(dataframe))
dataframe = subset(dataframe, User != "SavageAxeBot" & User != "KeepingDankMemesDank" & User != "AutoModerator"
& User != "DankMemesMods" & User != "Colorizebot" & User != "BattleBusBot" & User != "MemeInvestor_bot" & User != "Transcribot"
& User != "Transcribot" & User != "commonmisspellingbot" & User != "TiltedTowersBot" & User != "stormshieldonebot"
& User != "WikiTextBot" & User != "RemindMeBot" & User != "thank_mr_skeltal_bot" & User != "societybot"
& User != "rick_rolled_bot" & User != "NoSkinBot" & User != "REEEEEEEEEpost" & User != "2Fdankmemes" & User != "deleted" & User != "GlobalOffensiveBot")
print(count(dataframe))
dataFilter1_top <- dataframe %>%
dplyr::select(Comment) %>%
unnest_tokens(word, Comment)
#clean the top comments
datafilter1_top_clean <- dataFilter1_top %>%
filter(!word == "'") %>%
anti_join(as.data.frame(stop_words)) %>%
filter(!word == "shit") %>%
filter(!word == "x200b") %>%
filter(!word == "gt") %>%
filter(!word == "https") %>%
filter(!word == "it's") %>%
filter(!word == "amp") %>%
filter(!word == "fucking") %>%
filter(!word == "fuck") %>%
filter(!word == "1") %>%
filter(!word == "www.reddit.com") %>%
filter(!word == "2") %>%
filter(!word == "3") %>%
filter(!word == "removed") %>%
filter(!word == "deleted") %>%
filter(!word == "x200b")
dataFilter1_top_words <- datafilter1_top_clean %>%
count(word, sort = TRUE) %>%
top_n(200) %>%
mutate(word = reorder(word, n))
#word cloud
wordcloud(dataFilter1_top_words$word, dataFilter1_top_words$n, min.freq = 1,
max.words = 175, random.order = FALSE, rot.per = 0.25,
colors = brewer.pal(6, "Dark2"))
#freq graph
dataFilter1_top_words %>%
count(word, sort = TRUE) %>%
top_n(30) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "Words",
y = "Count",
title = paste("Most commonly used english words in ", subreddit))
Datacomment_data_paired <- dataframe %>%
dplyr::select(Comment) %>%
mutate(Comment = removeWords(Comment, stop_words$word)) %>%
mutate(Comment = gsub("\\brt\\b|\\bRT\\b", "", Comment)) %>%
mutate(Comment = gsub("http://*", "", Comment)) %>%
unnest_tokens(paired_words, Comment, token = "ngrams", n = 2)
data_Comments_Separated <- Datacomment_data_paired %>%
separate(paired_words, c("word1", "word2"), sep = " ") %>%
count(word1, word2, sort = TRUE)
head(data_Comments_Separated)
data_Comments_Separated %>%
filter(n >= minimumCountLinks) %>%
filter(!word1 == "shit") %>%
filter(!word1 == "gt") %>%
filter(!word1 == "https") %>%
filter(!word1 == "it's") %>%
filter(!word1 == "amp") %>%
filter(!word1 == "fucking") %>%
filter(!word1 == "fuck") %>%
filter(!word1 == "1") %>%
filter(!word1 == "2") %>%
filter(!word1 == "3") %>%
filter(!word1 == "removed") %>%
filter(!word1 == "deleted") %>%
filter(!word1 == "x200b") %>%
filter(!word2 == "shit") %>%
filter(!word2 == "gt") %>%
filter(!word2 == "https") %>%
filter(!word2 == "it's") %>%
filter(!word2 == "amp") %>%
filter(!word2 == "fucking") %>%
filter(!word2 == "fuck") %>%
filter(!word2 == "1") %>%
filter(!word2 == "2") %>%
filter(!word2 == "3") %>%
filter(!word2 == "removed") %>%
filter(!word2 == "deleted") %>%
filter(!word2 == "x200b") %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(colour = "darkslategray4", size = 4) +
geom_node_text(aes(label = name), vjust = 1.2, size = 2.4) +
labs(title = paste("Redditor Comment links on ", subreddit),
subtitle = "December 2018",
x = "", y = "") +
theme_void()
}
data = stream_in(file("data/dankmemes.json"), pagesize = 5000)
dankmemesAvgScore = mean(data$score)
dankmemesTotal = count(data)
createGraphs(data, "/r/DankMemes", 1000)
createGraphs(data, "/r/DankMemes", 1000)
#freq graph
dataFilter1_top_words %>%
count(word, sort = TRUE) %>%
top_n(30) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "Words",
y = "Count",
title = paste("Most commonly used english words in ", subreddit))
# Load
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("jsonlite")
library("NCmisc")
# plotting and pipes - tidyverse!
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidytext)
# coupled words analysis
library(widyr)
# plotting packages
library(igraph)
library(ggraph)
options(stringsAsFactors = FALSE)
dankMemesData = stream_in(file("data/dankmemes.json"), pagesize = 5000)
plot(dankMemesData$created_utc, dankMemesData$score)
dankMemesDataFrame <- data.frame(User = dankMemesData$author,
Date = as.POSIXct(dankMemesData$created_utc, origin='1970-01-01'),
Comment = dankMemesData$body,
Score = dankMemesData$score)
dankMemesFilter1 <- subset(dankMemesDataFrame, Date > "2018-12-01" & Date < "2018-12-07" & User != "SavageAxeBot
" & User != "KeepingDankMemesDank" & User != "AutoModerator" & User != "DankMemesMods")
head(dankMemesFilter1)
averagescore = mean(dankMemesFilter1$Score)
averagescore = round(averagescore, 2)
dankMemesFilter1_top <- dankMemesFilter1 %>%
dplyr::select(Comment) %>%
unnest_tokens(word, Comment)
dankMemes_top <- dankMemesDataFrame %>%
dplyr::select(Comment) %>%
unnest_tokens(word, Comment)
#clean the top comments
dankmemesfilter1_top_clean <- dankMemesFilter1_top %>%
anti_join(as.data.frame(stop_words)) %>%
filter(!word == "shit") %>%
filter(!word == "gt") %>%
filter(!word == "https") %>%
filter(!word == "it's") %>%
filter(!word == "amp") %>%
filter(!word == "fucking") %>%
filter(!word == "fuck") %>%
filter(!word == "1") %>%
filter(!word == "2") %>%
filter(!word == "3") %>%
filter(!word == "removed") %>%
filter(!word == "deleted")
dankmemes_top_clean <- dankMemes_top %>%
anti_join(as.data.frame(stop_words)) %>%
filter(!word == "shit") %>%
filter(!word == "gt") %>%
filter(!word == "https") %>%
filter(!word == "it's") %>%
filter(!word == "amp") %>%
filter(!word == "fucking") %>%
filter(!word == "fuck") %>%
filter(!word == "1") %>%
filter(!word == "2") %>%
filter(!word == "3") %>%
filter(!word == "removed") %>%
filter(!word == "deleted")
DankMemesFilter1_top_words <- dankmemesfilter1_top_clean %>%
count(word, sort = TRUE) %>%
top_n(200) %>%
mutate(word = reorder(word, n))
head(DankMemesFilter1_top_words)
#word cloud
wordcloud(DankMemesFilter1_top_words$word, DankMemesFilter1_top_words$n, min.freq = 1,
max.words = 200, random.order = FALSE, rot.per = 0.15,
colors = brewer.pal(8, "Dark2"))
#frequency graph
dankmemesfilter1_top_clean %>%
count(word, sort = TRUE) %>%
top_n(30) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "Words",
y = "Count",
title = "Most commonly used english words in /r/dankmemes, December 1st-7th 2018")
plot(dankMemesData$created_utc, dankMemesData$score)
dankMemesDataFrame <- data.frame(User = dankMemesData$author,
Date = as.POSIXct(dankMemesData$created_utc, origin='1970-01-01'),
Comment = dankMemesData$body,
Score = dankMemesData$score)
dankMemesFilter1 <- subset(dankMemesDataFrame, Date > "2018-12-01" & Date < "2018-12-07" & User != "SavageAxeBot
" & User != "KeepingDankMemesDank" & User != "AutoModerator" & User != "DankMemesMods"
& User != "REEEEEEEEEpost" & User != "2Fdankmemes" & User != "deleted")
head(dankMemesFilter1)
averagescore = mean(dankMemesFilter1$Score)
averagescore = round(averagescore, 2)
dankMemesFilter1_top <- dankMemesFilter1 %>%
dplyr::select(Comment) %>%
unnest_tokens(word, Comment)
dankMemes_top <- dankMemesDataFrame %>%
dplyr::select(Comment) %>%
unnest_tokens(word, Comment)
#clean the top comments
dankmemesfilter1_top_clean <- dankMemesFilter1_top %>%
anti_join(as.data.frame(stop_words)) %>%
filter(!word == "shit") %>%
filter(!word == "gt") %>%
filter(!word == "https") %>%
filter(!word == "it's") %>%
filter(!word == "amp") %>%
filter(!word == "fucking") %>%
filter(!word == "fuck") %>%
filter(!word == "1") %>%
filter(!word == "2") %>%
filter(!word == "3") %>%
filter(!word == "removed") %>%
filter(!word == "deleted")
dankmemes_top_clean <- dankMemes_top %>%
anti_join(as.data.frame(stop_words)) %>%
filter(!word == "shit") %>%
filter(!word == "x200b") %>%
filter(!word == "gt") %>%
filter(!word == "https") %>%
filter(!word == "it's") %>%
filter(!word == "amp") %>%
filter(!word == "fucking") %>%
filter(!word == "fuck") %>%
filter(!word == "1") %>%
filter(!word == "www.reddit.com") %>%
filter(!word == "2") %>%
filter(!word == "3") %>%
filter(!word == "removed") %>%
filter(!word == "deleted") %>%
filter(!word == "x200b")
DankMemesFilter1_top_words <- dankmemesfilter1_top_clean %>%
count(word, sort = TRUE) %>%
top_n(200) %>%
mutate(word = reorder(word, n))
head(DankMemesFilter1_top_words)
#word cloud
wordcloud(DankMemesFilter1_top_words$word, DankMemesFilter1_top_words$n, min.freq = 1,
max.words = 200, random.order = FALSE, rot.per = 0.15,
colors = brewer.pal(8, "Dark2"))
#frequency graph
dankmemesfilter1_top_clean %>%
count(word, sort = TRUE) %>%
top_n(30) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "Words",
y = "Count",
title = "Most commonly used english words in /r/dankmemes, December 1st-7th 2018")
# Load
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("jsonlite")
library("NCmisc")
# plotting and pipes - tidyverse!
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidytext)
# coupled words analysis
library(widyr)
# plotting packages
library(igraph)
library(ggraph)
options(stringsAsFactors = FALSE)
dankMemesData = stream_in(file("data/dankmemes.json"), pagesize = 5000)
plot(dankMemesData$created_utc, dankMemesData$score)
dankMemesDataFrame <- data.frame(User = dankMemesData$author,
Date = as.POSIXct(dankMemesData$created_utc, origin='1970-01-01'),
Comment = dankMemesData$body,
Score = dankMemesData$score)
dankMemesFilter1 <- subset(dankMemesDataFrame, Date > "2018-12-01" & Date < "2018-12-07" & User != "SavageAxeBot
" & User != "KeepingDankMemesDank" & User != "AutoModerator" & User != "DankMemesMods"
& User != "REEEEEEEEEpost" & User != "2Fdankmemes" & User != "deleted")
head(dankMemesFilter1)
averagescore = mean(dankMemesFilter1$Score)
averagescore = round(averagescore, 2)
dankMemesFilter1_top <- dankMemesFilter1 %>%
dplyr::select(Comment) %>%
unnest_tokens(word, Comment)
dankMemes_top <- dankMemesDataFrame %>%
dplyr::select(Comment) %>%
unnest_tokens(word, Comment)
#clean the top comments
dankmemesfilter1_top_clean <- dankMemesFilter1_top %>%
anti_join(as.data.frame(stop_words)) %>%
filter(!word == "shit") %>%
filter(!word == "x200b") %>%
filter(!word == "gt") %>%
filter(!word == "https") %>%
filter(!word == "it's") %>%
filter(!word == "amp") %>%
filter(!word == "fucking") %>%
filter(!word == "fuck") %>%
filter(!word == "1") %>%
filter(!word == "www.reddit.com") %>%
filter(!word == "2") %>%
filter(!word == "3") %>%
filter(!word == "removed") %>%
filter(!word == "deleted") %>%
filter(!word == "x200b")
dankmemes_top_clean <- dankMemes_top %>%
anti_join(as.data.frame(stop_words)) %>%
filter(!word == "shit") %>%
filter(!word == "x200b") %>%
filter(!word == "gt") %>%
filter(!word == "https") %>%
filter(!word == "it's") %>%
filter(!word == "amp") %>%
filter(!word == "fucking") %>%
filter(!word == "fuck") %>%
filter(!word == "1") %>%
filter(!word == "www.reddit.com") %>%
filter(!word == "2") %>%
filter(!word == "3") %>%
filter(!word == "removed") %>%
filter(!word == "deleted") %>%
filter(!word == "x200b")
DankMemesFilter1_top_words <- dankmemesfilter1_top_clean %>%
count(word, sort = TRUE) %>%
top_n(200) %>%
mutate(word = reorder(word, n))
head(DankMemesFilter1_top_words)
#word cloud
wordcloud(DankMemesFilter1_top_words$word, DankMemesFilter1_top_words$n, min.freq = 1,
max.words = 200, random.order = FALSE, rot.per = 0.15,
colors = brewer.pal(8, "Dark2"))
#frequency graph
dankmemesfilter1_top_clean %>%
count(word, sort = TRUE) %>%
top_n(30) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "Words",
y = "Count",
title = "Most commonly used english words in /r/dankmemes, December 1st-7th 2018")
